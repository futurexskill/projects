{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classifier_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG6vNyHEQoB1"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0KHwOZ0Q6jh",
        "outputId": "a20c89da-a546-4f33-a61e-d4f495a87ecf"
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0J_Sj_eRAO-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG0MHsxlRm0e"
      },
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/futurexskill/ml-model-deployment/main/Restaurant_Reviews.tsv.txt', delimiter= '\\t', quoting = 3)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Qj4QUwy7RtEi",
        "outputId": "fda77761-1281-4578-e02a-411b7114a4f8"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Liked\n",
              "0                           Wow... Loved this place.      1\n",
              "1                                 Crust is not good.      0\n",
              "2          Not tasty and the texture was just nasty.      0\n",
              "3  Stopped by during the late May bank holiday of...      1\n",
              "4  The selection on the menu was great and so wer...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Lb1G3NSZ0L"
      },
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogW8b3DTGQ8"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIwfo-1GTKWD"
      },
      "source": [
        "ps = PorterStemmer()\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCxuGsgPTM3_",
        "outputId": "d475b74e-34f1-4222-8f2a-245f4bdbfb5c"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  1000 non-null   object\n",
            " 1   Liked   1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MC79MlzTRgY"
      },
      "source": [
        "corpus = []\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUseJMNPVUzH"
      },
      "source": [
        "import re"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH7ziNMCTYiy"
      },
      "source": [
        "for i in range(0, 1000):\n",
        "\n",
        "  customer_review = re.sub('[^a-zA-Z]', ' ',dataset['Review'][i])\n",
        "  customer_review = customer_review.lower()\n",
        "  customer_review = customer_review.split()\n",
        "  clean_review = [ps.stem(word) for word in customer_review if not word in set(stopwords.words('english'))]\n",
        "  clean_review = ' '.join(clean_review)\n",
        "  corpus.append(clean_review)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Twuqh6fsVc6d",
        "outputId": "8710b757-e5dc-4e1e-bb9c-1aa00b05e948"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'wow love place'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DOr4rGoLVpAc",
        "outputId": "7ae0b87f-cddd-443f-fc48-dfa95b831958"
      },
      "source": [
        "corpus[6]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'honeslti tast fresh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AmFOCByVVwpm",
        "outputId": "4120389c-30ae-42cb-d58d-93489f00040d"
      },
      "source": [
        "corpus[12]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cashier care ever say still end wayyy overpr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGOH342dWDUU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features = 1500, min_df = 3, max_df = 0.6)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0uX9FMEWN-5"
      },
      "source": [
        "X = vectorizer.fit_transform(corpus).toarray()\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VJx99uhW3Uc",
        "outputId": "1b2dade2-724a-4ec5-f3a9-e872f724b1cc"
      },
      "source": [
        "X"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eN5zp88W4D9",
        "outputId": "cf601c24-71d8-4b45-905d-49bf6ac5cae8"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.51611335, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.37891311, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.76814834, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cg_taoYW_gA"
      },
      "source": [
        "y = dataset.iloc[:, 1].values"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Acy_ksXRkz",
        "outputId": "bc8a74c4-7535-4594-dd1f-818476bbe79e"
      },
      "source": [
        "y"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFwvHO6vXSQT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUHOUvsyS34"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_CqS6CVyM6B"
      },
      "source": [
        "Xtrain_ = torch.from_numpy(X_train).float()\n",
        "Xtest_ = torch.from_numpy(X_test).float()"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTp5us_HXXEw"
      },
      "source": [
        "ytrain_ = torch.from_numpy(y_train)\n",
        "ytest_ = torch.from_numpy(y_test)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HcNA4eDXee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fd078d-48f3-47e6-b3db-7a0624f51fda"
      },
      "source": [
        "Xtrain_.shape, ytrain_.shape"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([800, 467]), torch.Size([800]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZfiDEj3XnL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a7fab0-5e32-41a4-9429-1e19f06db9e6"
      },
      "source": [
        "Xtest_.shape, ytest_.shape"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([200, 467]), torch.Size([200]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXVUjwR6XqI9"
      },
      "source": [
        "input_size=467\n",
        "output_size=2\n",
        "hidden_size=500"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZgwPR8tXsiE"
      },
      "source": [
        "class Net(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(Net, self).__init__()\n",
        "       self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
        "       self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "       self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "   def forward(self, X):\n",
        "       X = torch.relu((self.fc1(X)))\n",
        "       X = torch.relu((self.fc2(X)))\n",
        "       X = self.fc3(X)\n",
        "\n",
        "       return F.log_softmax(X,dim=1)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c933ZQOyXuQd"
      },
      "source": [
        "model = Net()\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgnra037Xxl7"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCGzLzUHX7L_"
      },
      "source": [
        "epochs = 100\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYgpFHAbYDQf",
        "outputId": "88f71b25-bcc4-436e-a416-e59f2f1883bc"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  Ypred = model(Xtrain_)\n",
        "  loss = loss_fn(Ypred,  ytrain_)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print('Epoch',epoch, 'loss',loss.item())"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss 0.6928789019584656\n",
            "Epoch 1 loss 0.657316267490387\n",
            "Epoch 2 loss 0.4851149320602417\n",
            "Epoch 3 loss 0.3006068468093872\n",
            "Epoch 4 loss 0.1871432662010193\n",
            "Epoch 5 loss 0.1234399601817131\n",
            "Epoch 6 loss 0.08587438613176346\n",
            "Epoch 7 loss 0.0743032917380333\n",
            "Epoch 8 loss 0.05643957108259201\n",
            "Epoch 9 loss 0.04632105678319931\n",
            "Epoch 10 loss 0.0432513989508152\n",
            "Epoch 11 loss 0.04096271097660065\n",
            "Epoch 12 loss 0.036419786512851715\n",
            "Epoch 13 loss 0.03373660147190094\n",
            "Epoch 14 loss 0.03720105439424515\n",
            "Epoch 15 loss 0.032389603555202484\n",
            "Epoch 16 loss 0.03270404413342476\n",
            "Epoch 17 loss 0.03109397180378437\n",
            "Epoch 18 loss 0.03273000195622444\n",
            "Epoch 19 loss 0.03157077729701996\n",
            "Epoch 20 loss 0.02947317063808441\n",
            "Epoch 21 loss 0.03130174055695534\n",
            "Epoch 22 loss 0.03034614399075508\n",
            "Epoch 23 loss 0.029868917539715767\n",
            "Epoch 24 loss 0.029991598799824715\n",
            "Epoch 25 loss 0.030134165659546852\n",
            "Epoch 26 loss 0.028588488698005676\n",
            "Epoch 27 loss 0.02972126193344593\n",
            "Epoch 28 loss 0.02986041083931923\n",
            "Epoch 29 loss 0.029077135026454926\n",
            "Epoch 30 loss 0.028959054499864578\n",
            "Epoch 31 loss 0.029607605189085007\n",
            "Epoch 32 loss 0.029152393341064453\n",
            "Epoch 33 loss 0.028645062819123268\n",
            "Epoch 34 loss 0.0286861564964056\n",
            "Epoch 35 loss 0.028888888657093048\n",
            "Epoch 36 loss 0.028841914609074593\n",
            "Epoch 37 loss 0.028253702446818352\n",
            "Epoch 38 loss 0.02863980457186699\n",
            "Epoch 39 loss 0.029017414897680283\n",
            "Epoch 40 loss 0.028236771002411842\n",
            "Epoch 41 loss 0.028341105207800865\n",
            "Epoch 42 loss 0.028905155137181282\n",
            "Epoch 43 loss 0.028301112353801727\n",
            "Epoch 44 loss 0.02819119766354561\n",
            "Epoch 45 loss 0.0286888238042593\n",
            "Epoch 46 loss 0.028452640399336815\n",
            "Epoch 47 loss 0.028156399726867676\n",
            "Epoch 48 loss 0.028254471719264984\n",
            "Epoch 49 loss 0.028357286006212234\n",
            "Epoch 50 loss 0.028336040675640106\n",
            "Epoch 51 loss 0.028153814375400543\n",
            "Epoch 52 loss 0.028135566040873528\n",
            "Epoch 53 loss 0.028316516429185867\n",
            "Epoch 54 loss 0.028248701244592667\n",
            "Epoch 55 loss 0.028113892301917076\n",
            "Epoch 56 loss 0.02812819741666317\n",
            "Epoch 57 loss 0.028162576258182526\n",
            "Epoch 58 loss 0.0282213743776083\n",
            "Epoch 59 loss 0.028172776103019714\n",
            "Epoch 60 loss 0.0280712079256773\n",
            "Epoch 61 loss 0.02810998633503914\n",
            "Epoch 62 loss 0.028161492198705673\n",
            "Epoch 63 loss 0.028174763545393944\n",
            "Epoch 64 loss 0.028153350576758385\n",
            "Epoch 65 loss 0.028087876737117767\n",
            "Epoch 66 loss 0.028079409152269363\n",
            "Epoch 67 loss 0.028101488947868347\n",
            "Epoch 68 loss 0.028122730553150177\n",
            "Epoch 69 loss 0.02814888022840023\n",
            "Epoch 70 loss 0.0281336922198534\n",
            "Epoch 71 loss 0.028111588209867477\n",
            "Epoch 72 loss 0.028100106865167618\n",
            "Epoch 73 loss 0.02806984633207321\n",
            "Epoch 74 loss 0.02807210199534893\n",
            "Epoch 75 loss 0.028069842606782913\n",
            "Epoch 76 loss 0.028077244758605957\n",
            "Epoch 77 loss 0.028091924265027046\n",
            "Epoch 78 loss 0.028104137629270554\n",
            "Epoch 79 loss 0.028136037290096283\n",
            "Epoch 80 loss 0.028167767450213432\n",
            "Epoch 81 loss 0.028229545801877975\n",
            "Epoch 82 loss 0.028314601629972458\n",
            "Epoch 83 loss 0.028518764302134514\n",
            "Epoch 84 loss 0.028722237795591354\n",
            "Epoch 85 loss 0.02917522005736828\n",
            "Epoch 86 loss 0.029173560440540314\n",
            "Epoch 87 loss 0.029377920553088188\n",
            "Epoch 88 loss 0.028757713735103607\n",
            "Epoch 89 loss 0.028354255482554436\n",
            "Epoch 90 loss 0.028096860274672508\n",
            "Epoch 91 loss 0.028086034581065178\n",
            "Epoch 92 loss 0.028255613520741463\n",
            "Epoch 93 loss 0.028466418385505676\n",
            "Epoch 94 loss 0.028723960742354393\n",
            "Epoch 95 loss 0.02862025797367096\n",
            "Epoch 96 loss 0.02848186530172825\n",
            "Epoch 97 loss 0.028189532458782196\n",
            "Epoch 98 loss 0.028068898245692253\n",
            "Epoch 99 loss 0.02810860611498356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3cSNTmmYEg1"
      },
      "source": [
        "sample = [\"Good batting by England\"]\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3qftBXlYO3E"
      },
      "source": [
        "sample = vectorizer.transform(sample).toarray()\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpgyEuQCYTCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82984fa2-7812-4e5a-d13f-78c75def495c"
      },
      "source": [
        "sample"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZNMlfQyYYBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e4449d-e0c9-4ae5-fb69-1a7262b64970"
      },
      "source": [
        "torch.from_numpy(sample).float()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJkJtzwKYaoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88284106-a544-4994-de51-964563abd513"
      },
      "source": [
        "sentiment = model(torch.from_numpy(sample).float())\n",
        "sentiment"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7840, -0.1839]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frLMoG4kYfWX"
      },
      "source": [
        "sample2 = [\"bad performance by India in the match\"]\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcQ25_ne0K4w"
      },
      "source": [
        "sample2 = vectorizer.transform(sample2).toarray()\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIvI5szPYg0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f69856-6508-454a-d45f-6c42e3bf3ff9"
      },
      "source": [
        "sentiment2 = model(torch.from_numpy(sample2).float())\n",
        "sentiment2"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.0000, -48.3463]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkZbp_CC0YTg",
        "outputId": "3846146f-115f-4a99-8c64-6e09420620c1"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.weight',\n",
              "              tensor([[-0.0134, -0.0340,  0.0075,  ..., -0.0249,  0.0426, -0.0791],\n",
              "                      [-0.0360, -0.0350,  0.0074,  ..., -0.0282, -0.0278, -0.0683],\n",
              "                      [-0.1294, -0.0165,  0.1079,  ...,  0.0007,  0.0449, -0.0858],\n",
              "                      ...,\n",
              "                      [ 0.0826, -0.0311, -0.0935,  ...,  0.0890,  0.1167, -0.0398],\n",
              "                      [ 0.0894,  0.1033, -0.0218,  ...,  0.0719,  0.0133,  0.1212],\n",
              "                      [ 0.0402, -0.0295, -0.0941,  ...,  0.0731,  0.1296, -0.0440]])),\n",
              "             ('fc1.bias',\n",
              "              tensor([ 0.0091, -0.0883, -0.0259, -0.0132, -0.0178, -0.0710,  0.0129, -0.0792,\n",
              "                      -0.0152,  0.0137,  0.0092, -0.0950,  0.0467, -0.1021, -0.0263, -0.0181,\n",
              "                       0.0557, -0.0077, -0.0185,  0.0364, -0.0438,  0.0196,  0.0595, -0.0079,\n",
              "                      -0.0748, -0.0359, -0.0065,  0.0994, -0.0105, -0.1196, -0.0672,  0.0128,\n",
              "                       0.0543, -0.0178,  0.0133,  0.0054, -0.0082,  0.0293, -0.0897, -0.0807,\n",
              "                      -0.0529,  0.0017, -0.0149, -0.0320, -0.0189, -0.0320,  0.0089, -0.0144,\n",
              "                       0.0095,  0.0440,  0.0040, -0.0031,  0.0077, -0.0837, -0.0886, -0.0044,\n",
              "                      -0.0021,  0.0086, -0.0476, -0.0161, -0.1007,  0.0457, -0.0674, -0.0126,\n",
              "                       0.0298, -0.0147, -0.0230, -0.0096,  0.0429,  0.0139,  0.0600, -0.0152,\n",
              "                       0.0131, -0.0034,  0.0018, -0.0239, -0.0295, -0.0229,  0.0432,  0.0076,\n",
              "                      -0.0800, -0.0125, -0.0229, -0.0333,  0.0216,  0.0140, -0.0688, -0.0175,\n",
              "                      -0.0147, -0.0281, -0.0563,  0.0151,  0.0362,  0.0171,  0.0205,  0.0240,\n",
              "                      -0.0095, -0.0433,  0.0067,  0.0747,  0.0433,  0.0548, -0.0183,  0.0140,\n",
              "                       0.0504,  0.0552,  0.0135, -0.0072,  0.0373, -0.0271, -0.0890,  0.0162,\n",
              "                      -0.0222, -0.0283, -0.0095, -0.0013,  0.0580, -0.0890, -0.0492,  0.0141,\n",
              "                      -0.0872,  0.0025, -0.0613, -0.0168,  0.0353, -0.0072,  0.0355,  0.0501,\n",
              "                       0.0256, -0.0401,  0.0398, -0.0716, -0.0691, -0.0436, -0.0757, -0.0528,\n",
              "                       0.0330,  0.0478, -0.0197,  0.0254, -0.0087, -0.1017, -0.0994,  0.0142,\n",
              "                       0.0335,  0.0271,  0.0137, -0.0068, -0.0155, -0.0024, -0.0223, -0.0132,\n",
              "                       0.0489, -0.0223,  0.0243,  0.0502, -0.0657, -0.0136, -0.0211, -0.0825,\n",
              "                       0.0268,  0.0289, -0.0608, -0.0296, -0.0954, -0.0226,  0.0453, -0.1006,\n",
              "                       0.0088,  0.0424,  0.0666, -0.0090, -0.0280,  0.0039,  0.0115,  0.0143,\n",
              "                      -0.0185, -0.1035, -0.0434,  0.0383, -0.0070,  0.0340, -0.0228, -0.0146,\n",
              "                       0.0281,  0.0384, -0.1046, -0.0140, -0.0039,  0.0088, -0.0147,  0.0535,\n",
              "                       0.0145,  0.0167,  0.0192, -0.0449,  0.0327, -0.0110, -0.0314, -0.0347,\n",
              "                       0.0048,  0.0317,  0.0058, -0.0175, -0.0178, -0.0292,  0.0472, -0.0324,\n",
              "                      -0.0467,  0.0369, -0.0040, -0.0321, -0.0152,  0.0099,  0.0085,  0.0287,\n",
              "                       0.0591,  0.0214, -0.0910, -0.1069, -0.0167, -0.0375, -0.0090, -0.0273,\n",
              "                      -0.0457, -0.1136, -0.0263,  0.0418,  0.0279,  0.0469, -0.0535,  0.0170,\n",
              "                      -0.0151,  0.0038,  0.0470, -0.1192, -0.0051,  0.0409,  0.0132,  0.0163,\n",
              "                       0.0005,  0.0237,  0.0127, -0.0475,  0.0334,  0.0266, -0.0037,  0.0128,\n",
              "                       0.0276, -0.0175,  0.0288,  0.0330, -0.0053, -0.0885,  0.0196, -0.1226,\n",
              "                      -0.1122, -0.0282, -0.0188, -0.0294, -0.0260, -0.0266, -0.0052,  0.0431,\n",
              "                      -0.0018, -0.0108,  0.0165, -0.0825, -0.0317, -0.0611,  0.0137,  0.0436,\n",
              "                       0.0506,  0.0385, -0.0180,  0.0093, -0.1259,  0.0053,  0.0276,  0.0108,\n",
              "                      -0.0435,  0.0390, -0.0069, -0.0851, -0.0242,  0.0324, -0.0128, -0.0179,\n",
              "                      -0.0297, -0.0114,  0.0264, -0.0201,  0.0464, -0.0078, -0.0411,  0.0323,\n",
              "                      -0.0792, -0.0356,  0.0188, -0.0276,  0.0256, -0.0157,  0.0269,  0.0438,\n",
              "                       0.0108,  0.0006,  0.0120, -0.0560, -0.0381, -0.0071, -0.0842, -0.0876,\n",
              "                      -0.0155, -0.0390, -0.0159, -0.0123, -0.0055, -0.0636, -0.0154,  0.0127,\n",
              "                      -0.0426,  0.0220, -0.1191, -0.0870, -0.0278, -0.0018,  0.0009, -0.0364,\n",
              "                      -0.0165, -0.0209, -0.0069,  0.0312, -0.0219,  0.0364, -0.0079,  0.0044,\n",
              "                       0.0111,  0.0123, -0.0619, -0.0636,  0.0396, -0.0180, -0.0217,  0.0167,\n",
              "                      -0.0161, -0.0179, -0.0038,  0.0535, -0.0150,  0.0421, -0.1095, -0.0727,\n",
              "                       0.0054, -0.0954, -0.0545,  0.0325, -0.0188,  0.0265,  0.0128, -0.0049,\n",
              "                       0.0623, -0.0106,  0.0323,  0.0346,  0.0914, -0.0096, -0.0875,  0.0233,\n",
              "                      -0.0096,  0.0092, -0.0072, -0.0102, -0.0435,  0.0485, -0.0035,  0.0125,\n",
              "                      -0.0292,  0.0539, -0.0197,  0.0310,  0.0263, -0.0577, -0.0330, -0.1168,\n",
              "                       0.0254, -0.0979,  0.0196,  0.0121,  0.0465, -0.0973, -0.1098,  0.0471,\n",
              "                       0.0112, -0.0073,  0.0304, -0.0104,  0.0442, -0.0122, -0.0158, -0.0300,\n",
              "                       0.0321, -0.0242,  0.0256,  0.0183, -0.0895, -0.0179, -0.0644, -0.0025,\n",
              "                       0.0066,  0.0091,  0.0271,  0.0668, -0.0944,  0.0040,  0.0536,  0.0814,\n",
              "                       0.0043, -0.1271, -0.0062, -0.0196,  0.0018,  0.0173, -0.0320,  0.0390,\n",
              "                      -0.0059, -0.0122,  0.0670, -0.0225,  0.0082,  0.0536,  0.0555, -0.0980,\n",
              "                       0.0360,  0.0324, -0.0315,  0.0415,  0.0164,  0.0037, -0.0970,  0.0033,\n",
              "                      -0.0068,  0.0452, -0.0161,  0.0283, -0.0278,  0.0648, -0.0429, -0.0933,\n",
              "                       0.0196, -0.0015, -0.1173, -0.0879,  0.0174,  0.0197, -0.0116, -0.0398,\n",
              "                       0.0263,  0.0029, -0.0527,  0.0476,  0.0336,  0.0143,  0.0470, -0.0890,\n",
              "                       0.0355, -0.0297,  0.0164, -0.0827,  0.0489,  0.0226,  0.0421,  0.0045,\n",
              "                      -0.0322,  0.0005,  0.0067, -0.0194, -0.0794,  0.0088,  0.0029,  0.0183,\n",
              "                       0.0365,  0.0075,  0.0536, -0.1169,  0.0245, -0.0598, -0.0479,  0.0111,\n",
              "                      -0.0756,  0.0521, -0.0264, -0.0645,  0.0080,  0.0044,  0.0099, -0.0310,\n",
              "                       0.0583,  0.0438,  0.0264, -0.0300])),\n",
              "             ('fc2.weight',\n",
              "              tensor([[ 0.0423, -0.0257, -0.1075,  ...,  0.0228,  0.0955, -0.0637],\n",
              "                      [ 0.1126,  0.0596, -0.1160,  ...,  0.0270,  0.1145, -0.0936],\n",
              "                      [ 0.0569,  0.0020, -0.1080,  ...,  0.0379,  0.1328, -0.0867],\n",
              "                      ...,\n",
              "                      [ 0.0498, -0.0480, -0.0257,  ..., -0.0858, -0.0464, -0.0947],\n",
              "                      [-0.0478,  0.0884,  0.1045,  ..., -0.0682, -0.0642,  0.0539],\n",
              "                      [-0.0603,  0.0866,  0.0913,  ...,  0.0198, -0.0665,  0.0410]])),\n",
              "             ('fc2.bias',\n",
              "              tensor([ 1.0162e-01,  3.8784e-02,  7.7273e-02, -4.2499e-02,  8.7816e-02,\n",
              "                       7.8608e-02, -1.0058e-01, -4.3268e-02,  4.9523e-02,  3.8660e-02,\n",
              "                      -7.3525e-02, -7.2282e-02, -1.7793e-02,  9.4667e-02, -3.9902e-02,\n",
              "                       7.6845e-02, -2.4016e-02,  7.2330e-02, -8.9229e-02, -4.4487e-02,\n",
              "                      -2.4040e-02, -8.0051e-02, -6.1762e-02, -7.5675e-02, -4.6269e-02,\n",
              "                      -2.5890e-02, -8.7863e-02,  5.0233e-02,  6.5191e-02,  5.0222e-02,\n",
              "                       4.7730e-02,  5.7912e-02, -3.3417e-02,  7.8003e-02,  7.8774e-02,\n",
              "                       8.9634e-02, -1.7867e-02, -4.7430e-02, -2.6076e-02, -6.6963e-02,\n",
              "                       7.9786e-02, -7.6034e-02,  7.5817e-02,  6.8224e-02,  7.4004e-02,\n",
              "                      -9.8890e-02, -1.3855e-03,  1.0610e-02,  9.9473e-02, -9.3431e-02,\n",
              "                      -9.5366e-02,  2.7181e-03,  2.3562e-02,  4.3188e-02, -2.4428e-02,\n",
              "                       8.2913e-02, -7.8176e-02,  6.0158e-02, -5.1098e-02,  4.8388e-02,\n",
              "                       5.1422e-02,  1.1110e-02,  6.9299e-02,  7.8379e-02, -3.1813e-02,\n",
              "                      -5.2432e-02, -1.1240e-02,  8.1802e-02, -4.3149e-02,  5.8042e-02,\n",
              "                      -5.5815e-02,  7.3175e-02, -4.6723e-02,  1.0127e-01,  6.5165e-02,\n",
              "                       6.0258e-02, -1.6741e-02,  6.4599e-02,  6.0488e-03, -4.1014e-02,\n",
              "                      -4.4541e-02,  7.8147e-02, -4.4458e-03,  8.4538e-02, -2.0277e-02,\n",
              "                       1.2766e-02, -5.1829e-02, -8.8344e-02, -4.0562e-02,  8.6279e-02,\n",
              "                       5.5646e-03,  2.7927e-02,  6.1469e-02, -9.6865e-02,  8.6090e-02,\n",
              "                       9.0737e-02,  9.5067e-02, -5.9862e-02, -6.4785e-02,  7.8894e-02,\n",
              "                       7.3775e-02,  2.7103e-02,  2.7394e-02,  7.3321e-02,  6.8368e-02,\n",
              "                       5.3386e-02, -3.4462e-02, -3.8428e-02,  8.7168e-02,  9.2047e-02,\n",
              "                       7.0092e-02,  3.3837e-02, -4.7564e-02,  3.8269e-02,  6.8804e-02,\n",
              "                      -8.0637e-02,  1.0033e-02,  8.7968e-02, -2.5640e-02, -9.6244e-02,\n",
              "                       5.7407e-02, -6.0829e-02, -7.6099e-02,  1.1186e-02,  1.0754e-01,\n",
              "                       3.7124e-02,  4.8136e-02, -7.1019e-02, -6.8308e-02,  9.5613e-02,\n",
              "                       5.5886e-03,  1.0498e-02,  5.3282e-02, -1.1707e-02,  1.0250e-01,\n",
              "                      -7.1263e-02, -7.3518e-02, -6.2753e-02, -8.1450e-02, -6.2200e-02,\n",
              "                       4.9557e-02,  9.2813e-02,  6.4152e-02, -7.8315e-02, -9.0936e-02,\n",
              "                       6.7041e-02,  7.9993e-02,  1.3938e-02,  1.4986e-02, -2.8716e-02,\n",
              "                      -7.4830e-02,  6.1238e-03,  5.7328e-02, -3.0719e-02,  8.4897e-02,\n",
              "                       2.5716e-02,  5.8223e-03, -4.7164e-02,  5.2057e-02,  7.9867e-02,\n",
              "                       4.8856e-02,  6.6133e-02, -6.1497e-02, -1.4323e-02,  7.2319e-02,\n",
              "                       6.7078e-02,  9.1844e-02, -1.1052e-03, -3.0795e-03, -8.8421e-02,\n",
              "                      -3.1469e-02,  4.3872e-02, -2.9969e-02,  4.0183e-02,  1.4653e-02,\n",
              "                      -6.7514e-02, -4.0716e-02,  7.6062e-02,  8.4111e-02,  2.3537e-02,\n",
              "                      -8.3446e-02,  3.3962e-02, -8.3157e-02, -3.2923e-02,  7.2760e-02,\n",
              "                       4.7992e-02,  8.3143e-02,  8.7367e-02, -3.5911e-02,  1.1045e-01,\n",
              "                      -8.0832e-02, -8.2377e-02, -3.7017e-02,  5.0694e-02,  9.6987e-02,\n",
              "                      -7.2735e-02, -9.0366e-02,  1.9112e-02,  4.5114e-02, -5.8530e-02,\n",
              "                       1.7596e-02, -1.0421e-01, -9.1114e-02, -7.7897e-02, -6.1823e-02,\n",
              "                       7.6008e-02, -3.4621e-02,  1.9223e-02, -4.3318e-02, -4.9089e-02,\n",
              "                       3.7446e-02,  8.7984e-02, -5.9890e-02,  3.0606e-02,  8.4111e-02,\n",
              "                       8.2468e-02,  9.5295e-03,  7.1131e-02,  4.8892e-02,  3.2666e-03,\n",
              "                      -7.6364e-02,  6.8253e-02, -5.4390e-02,  5.9857e-02, -8.3604e-02,\n",
              "                      -4.6730e-03, -7.1273e-02,  7.0764e-02,  9.1595e-02, -9.4866e-02,\n",
              "                       8.5582e-02,  2.3043e-02,  9.4734e-02,  7.3495e-02, -5.3166e-02,\n",
              "                      -7.0179e-02,  1.4628e-02,  8.8886e-02, -3.7209e-02,  1.5682e-02,\n",
              "                      -9.5331e-02, -5.2565e-02, -9.7065e-02, -7.7787e-02, -6.2979e-03,\n",
              "                       8.2787e-04,  7.8979e-02, -7.9077e-02,  5.4892e-02, -9.1440e-04,\n",
              "                       2.6911e-02, -6.7636e-02, -8.1084e-02, -8.8443e-02, -7.8111e-02,\n",
              "                       7.8157e-02, -6.4703e-02,  8.4266e-02, -8.3188e-02, -3.6952e-02,\n",
              "                       1.9961e-02, -4.3656e-02,  3.8056e-02, -2.4174e-02, -8.9637e-02,\n",
              "                       9.1304e-02,  5.5326e-02,  6.2000e-02,  3.6562e-02,  1.2129e-02,\n",
              "                       9.5163e-02, -6.2591e-02,  1.6237e-02, -8.3426e-02, -6.3899e-02,\n",
              "                      -8.6732e-02, -4.9970e-02,  4.1285e-02,  8.3303e-02,  7.5771e-02,\n",
              "                      -3.9939e-02, -6.2053e-02, -6.2151e-02, -4.0818e-02,  4.3611e-02,\n",
              "                       5.3290e-02, -8.3665e-02, -9.7709e-02,  2.1512e-02,  7.8244e-02,\n",
              "                      -8.6653e-02,  2.8977e-02, -4.1131e-02,  5.0597e-02,  9.1342e-02,\n",
              "                      -6.6000e-02,  8.1355e-02, -3.4138e-03,  6.8506e-03,  4.9270e-02,\n",
              "                      -9.1973e-02, -4.5438e-02,  7.6641e-02, -5.3949e-02,  5.7132e-02,\n",
              "                      -4.5244e-02,  1.7282e-02, -8.6628e-02, -7.1743e-02, -8.9669e-02,\n",
              "                       8.2085e-02,  7.8164e-02,  2.5227e-02,  1.0840e-01, -4.1757e-02,\n",
              "                       1.7302e-02,  1.1503e-01,  7.3916e-02, -5.7007e-02,  1.0617e-02,\n",
              "                      -5.6986e-02, -7.2828e-02,  1.7674e-03,  4.7537e-02,  4.2388e-02,\n",
              "                       1.3852e-02, -3.2528e-02, -6.6163e-02,  7.6653e-02,  5.1628e-02,\n",
              "                      -9.0418e-02, -6.4963e-02,  9.6854e-02,  5.4560e-02, -5.8273e-02,\n",
              "                       4.8327e-04,  9.4649e-02, -6.5713e-02,  6.5475e-02,  6.5841e-02,\n",
              "                       8.1979e-02,  6.2563e-02,  9.3955e-02, -3.3780e-02, -9.5752e-02,\n",
              "                       2.6174e-02,  8.6633e-02,  1.6871e-02,  6.8509e-02,  7.1031e-02,\n",
              "                       2.2309e-02,  7.4054e-02, -9.6343e-02, -2.3513e-02,  1.0135e-01,\n",
              "                       7.2210e-02, -5.0123e-03,  7.9946e-03, -5.0890e-02, -9.9256e-02,\n",
              "                       2.4482e-02,  7.2102e-02, -8.2246e-02,  7.2024e-02,  6.8915e-02,\n",
              "                       1.7532e-02, -5.5288e-02, -4.6347e-02,  2.7056e-02,  8.4861e-02,\n",
              "                       6.3688e-02,  7.4244e-02, -2.5207e-02,  6.1610e-02,  1.8888e-02,\n",
              "                      -3.8455e-02, -8.6466e-02,  4.3835e-02,  7.4461e-02, -3.7979e-02,\n",
              "                       4.0001e-02,  2.4479e-02,  9.3520e-02,  7.9098e-02,  4.0351e-02,\n",
              "                       6.5353e-03,  9.9071e-02,  5.9611e-02,  3.4991e-02,  9.0100e-02,\n",
              "                       4.0351e-02,  6.7862e-02,  6.7192e-02, -4.7589e-02,  7.6534e-02,\n",
              "                       2.4947e-02, -5.5327e-02,  7.1260e-02, -1.1902e-01,  7.9459e-02,\n",
              "                      -6.2969e-02, -3.6689e-02, -7.1229e-02,  5.8805e-03,  8.1317e-02,\n",
              "                      -8.9164e-02,  2.0018e-02, -2.9880e-02, -6.8077e-02,  7.8777e-02,\n",
              "                      -7.9801e-02, -6.0453e-02,  1.3639e-02, -9.5666e-02,  3.2573e-02,\n",
              "                      -8.4289e-02,  8.3015e-02,  4.5408e-02,  4.0507e-02, -5.6073e-02,\n",
              "                      -3.8415e-02,  5.3351e-03, -4.8955e-02, -8.9024e-02, -4.0308e-02,\n",
              "                       3.0722e-02,  7.7517e-03,  8.9705e-06, -7.6252e-02, -3.8707e-02,\n",
              "                      -1.2558e-02,  5.4116e-02, -2.6413e-02, -5.8989e-02, -4.3713e-02,\n",
              "                      -2.0027e-02,  9.4182e-02, -9.2404e-02,  5.7005e-02,  7.0022e-03,\n",
              "                       8.1126e-02,  6.1747e-02, -8.4807e-02,  8.8227e-02,  9.0768e-03,\n",
              "                       8.8962e-02, -7.7762e-02, -4.1810e-02,  9.3030e-02, -4.9292e-02,\n",
              "                       9.3315e-03,  9.8996e-02,  5.6494e-02, -8.4822e-02, -7.8696e-02,\n",
              "                      -3.2712e-02, -1.8833e-02, -3.9874e-02, -8.7341e-02, -7.5853e-02,\n",
              "                       8.4389e-02,  9.1855e-02,  8.2914e-02,  3.3467e-02,  5.0292e-02,\n",
              "                      -5.2470e-02,  7.9868e-02, -8.7011e-02,  1.3667e-02,  2.8042e-03,\n",
              "                      -7.2546e-02, -1.8512e-02, -1.1619e-02, -1.0050e-01,  7.3759e-02,\n",
              "                       1.9953e-02,  2.5179e-02,  5.0934e-03,  6.8652e-02, -5.1371e-03,\n",
              "                       6.2677e-02,  2.7614e-02,  7.8228e-02, -6.2656e-02, -9.1545e-02,\n",
              "                       8.5662e-02,  5.8534e-02,  4.1793e-02,  1.1430e-01, -8.9335e-02,\n",
              "                       4.9169e-02,  4.4344e-02,  5.5430e-02, -4.2168e-02,  3.2428e-02,\n",
              "                       7.6691e-02,  5.3331e-02, -7.7882e-02, -3.5162e-02,  3.8130e-02])),\n",
              "             ('fc3.weight',\n",
              "              tensor([[ 9.7527e-02,  4.0017e-02,  5.5258e-02,  5.0669e-02, -1.3947e-01,\n",
              "                        6.8373e-02,  1.4659e-02, -3.3738e-02, -1.0158e-01,  8.3338e-02,\n",
              "                        5.2903e-02,  3.5927e-02, -4.3288e-02, -1.1446e-01, -5.5818e-02,\n",
              "                        7.5389e-02, -2.1096e-02,  7.7238e-02,  3.7392e-02,  4.3237e-02,\n",
              "                       -7.6483e-02, -2.6480e-02, -3.6968e-02,  6.1079e-02, -3.9370e-02,\n",
              "                        2.6311e-02, -3.7524e-02,  9.0519e-02, -4.5569e-02,  8.8595e-02,\n",
              "                        8.8434e-02,  1.1108e-01,  2.3882e-02,  1.0476e-01, -8.6154e-02,\n",
              "                       -9.9016e-02, -4.9859e-02, -1.2088e-02, -8.1470e-02,  3.9182e-02,\n",
              "                        9.7184e-02,  1.9124e-02, -1.0842e-01,  9.7142e-02, -6.4230e-02,\n",
              "                       -8.0724e-02, -1.1576e-01,  2.7580e-04,  7.9701e-02,  2.2062e-02,\n",
              "                        3.1547e-02,  3.3744e-03, -1.7105e-02,  7.2765e-02, -1.0860e-02,\n",
              "                       -9.4287e-02,  2.9057e-02,  6.1625e-02, -1.9994e-02, -9.3376e-02,\n",
              "                       -1.1525e-01,  1.6618e-02, -1.1494e-01, -1.0006e-01,  4.3636e-02,\n",
              "                        1.0692e-02, -4.1126e-02, -7.3383e-02,  1.5359e-02,  9.2305e-02,\n",
              "                        6.2116e-02,  5.9970e-02,  6.7245e-02,  1.3406e-01, -2.9886e-02,\n",
              "                       -9.4111e-02,  1.2750e-02, -2.4676e-02, -3.5448e-02,  2.2741e-02,\n",
              "                        2.1675e-02, -6.3716e-02,  2.7739e-02, -1.2005e-01, -1.0033e-01,\n",
              "                        1.3395e-02, -8.0063e-02, -2.4616e-02, -4.6297e-02, -1.3284e-01,\n",
              "                        1.1003e-01, -4.9721e-02, -8.2781e-02, -3.6046e-02, -9.9854e-02,\n",
              "                       -1.2209e-01, -1.4116e-01,  7.5994e-02,  5.4083e-02,  7.7531e-02,\n",
              "                       -1.1515e-01, -6.0328e-02,  6.5261e-02, -1.3587e-01, -1.1990e-01,\n",
              "                        1.2048e-01,  2.2640e-02,  5.0912e-02,  1.0972e-01,  1.2226e-01,\n",
              "                       -1.0477e-01, -1.0114e-01, -1.0517e-01,  8.8288e-02, -7.6276e-02,\n",
              "                        6.1427e-02,  8.3045e-02, -1.1976e-01, -3.8382e-02,  4.7995e-02,\n",
              "                       -1.0338e-01,  5.2349e-02,  5.6462e-02, -6.7700e-02,  1.0510e-01,\n",
              "                        8.6826e-02,  9.5481e-02,  2.2822e-02, -4.1436e-02,  1.2602e-01,\n",
              "                        7.1294e-02, -6.8005e-02,  7.4835e-02, -2.7662e-02,  1.3421e-01,\n",
              "                       -4.2348e-03, -3.2693e-02,  6.4953e-02, -1.5225e-02, -3.9905e-03,\n",
              "                        1.0144e-01,  1.2977e-01, -1.1920e-01,  3.7749e-02, -4.1602e-02,\n",
              "                        1.2830e-01,  1.2322e-01, -7.5933e-02,  4.2996e-02,  1.6260e-02,\n",
              "                       -3.6121e-02,  2.5213e-02, -9.3603e-02,  3.1713e-02,  1.1248e-01,\n",
              "                        1.1823e-01, -1.0163e-01,  4.7395e-02,  9.8964e-02, -6.9997e-02,\n",
              "                        1.3469e-01,  9.5174e-02, -7.4782e-02,  8.8362e-03, -1.4055e-01,\n",
              "                       -1.0818e-01,  6.7133e-02,  2.9352e-02,  1.9060e-02,  5.2637e-03,\n",
              "                       -3.1303e-02, -8.1883e-02, -2.8042e-02,  1.0472e-01,  6.4650e-02,\n",
              "                        2.6259e-02, -3.3837e-02,  1.0068e-01, -8.3981e-02, -1.0051e-01,\n",
              "                       -2.1662e-02,  6.0718e-02, -1.9932e-02,  3.3669e-02, -1.3173e-01,\n",
              "                       -1.3544e-01, -1.1127e-01, -1.2892e-01, -4.8529e-03, -1.1136e-01,\n",
              "                        6.6039e-02, -7.9924e-02, -7.8098e-02,  8.0381e-02,  8.9095e-02,\n",
              "                       -1.0335e-02, -6.3819e-02,  7.6337e-02,  9.6611e-02,  3.7078e-02,\n",
              "                        8.5261e-02,  9.0895e-03,  2.1456e-02,  4.5224e-02, -4.1016e-02,\n",
              "                        1.2984e-01, -4.2716e-02,  1.1322e-01,  1.9058e-02, -6.8129e-02,\n",
              "                       -9.2094e-02, -8.7384e-02,  4.7660e-02,  1.1964e-01,  1.0414e-01,\n",
              "                        1.2383e-01, -2.7589e-02,  7.5055e-02, -7.5133e-02,  3.6378e-02,\n",
              "                        8.5788e-02, -9.2413e-02, -1.5437e-03, -7.4226e-02,  2.0637e-02,\n",
              "                       -7.3345e-02, -2.9810e-02, -1.3432e-01,  1.1738e-01,  6.3745e-02,\n",
              "                        7.6632e-02, -6.8278e-02,  8.0420e-02, -1.0043e-01, -8.0860e-02,\n",
              "                       -3.2909e-02,  6.4088e-02, -1.2615e-01, -4.0827e-02,  9.3634e-02,\n",
              "                        2.8020e-02, -3.9113e-02,  5.5111e-02,  1.4532e-06, -4.2369e-02,\n",
              "                       -1.1680e-01, -6.2006e-02, -6.6244e-02,  1.0530e-01, -8.9098e-02,\n",
              "                        7.2284e-02, -5.6118e-02,  2.8752e-02, -8.0468e-02,  2.7040e-02,\n",
              "                       -9.3418e-02,  5.4603e-02,  8.7257e-02, -4.2623e-02,  6.1745e-02,\n",
              "                        9.4067e-02, -8.6155e-02, -1.2482e-01,  3.1109e-02, -5.5251e-02,\n",
              "                       -9.9961e-02,  7.5303e-02, -9.1520e-02, -8.5596e-02,  6.6093e-02,\n",
              "                        5.1475e-02, -5.8680e-02,  4.4505e-02,  3.3462e-02,  7.0537e-02,\n",
              "                       -2.2966e-02,  8.0845e-02, -2.6124e-02, -1.2065e-01,  7.8918e-02,\n",
              "                       -8.5371e-02, -4.3091e-02,  6.4285e-02,  9.1894e-02, -1.1078e-02,\n",
              "                        7.4303e-02,  2.6512e-02,  1.8026e-02,  6.0933e-02, -1.3583e-01,\n",
              "                       -8.4168e-02,  1.8659e-02,  2.5358e-02,  1.2297e-01,  9.7883e-02,\n",
              "                        6.5508e-02, -9.5582e-02, -4.3957e-02,  2.0239e-02,  9.6550e-02,\n",
              "                       -7.3215e-03, -2.3274e-02,  4.9136e-02, -1.7029e-02, -7.2774e-02,\n",
              "                       -2.2708e-02,  9.0169e-02,  6.5383e-02, -2.9768e-02,  3.4786e-02,\n",
              "                       -1.1180e-01,  9.6958e-02, -5.4470e-02,  1.1138e-01,  6.9744e-02,\n",
              "                        9.9713e-02,  1.0120e-01, -5.8675e-02, -2.3334e-02, -1.1265e-02,\n",
              "                       -7.9896e-02, -3.3212e-02, -7.6858e-02,  7.0700e-02,  7.7083e-02,\n",
              "                        7.4120e-02,  2.3290e-03,  5.8218e-02,  8.2607e-02, -1.1040e-01,\n",
              "                        2.8869e-02, -7.8541e-02,  1.2088e-01, -1.2056e-01, -4.7040e-02,\n",
              "                        7.8480e-02,  1.1930e-01,  4.7970e-02,  7.0030e-02, -1.0256e-01,\n",
              "                       -7.1747e-02, -1.3135e-01,  1.1777e-01,  5.9006e-03, -1.9715e-03,\n",
              "                        3.1043e-02, -1.3327e-01,  5.4668e-02,  8.5746e-02, -5.4179e-02,\n",
              "                        3.6969e-02, -1.3053e-01, -6.4053e-02, -1.1805e-02,  1.3514e-01,\n",
              "                        1.2345e-01,  1.0783e-01, -8.1867e-02,  3.5285e-02,  7.4092e-02,\n",
              "                        3.8047e-02, -1.3143e-01, -6.2615e-02,  4.6266e-02,  7.6593e-02,\n",
              "                       -7.8788e-02, -3.4518e-02, -8.8958e-02,  8.5984e-02, -9.4390e-02,\n",
              "                        6.0133e-02,  1.0912e-01, -8.4155e-02,  4.4413e-02, -1.2305e-01,\n",
              "                        6.4263e-02, -1.1694e-02, -6.9252e-02, -1.0271e-01, -1.5696e-02,\n",
              "                        1.1134e-01,  1.0990e-01, -1.2948e-01, -1.3522e-01,  8.2600e-02,\n",
              "                        9.8413e-02,  1.1519e-01, -8.5694e-02,  8.0421e-02, -1.0278e-01,\n",
              "                        6.4092e-02, -9.3510e-02, -1.0239e-01, -6.1556e-02, -1.1538e-01,\n",
              "                        7.8476e-02, -7.0473e-02,  9.8565e-02,  3.1455e-02,  1.0328e-01,\n",
              "                        6.4988e-02,  2.7346e-02, -2.5307e-02,  4.4424e-02,  8.0175e-02,\n",
              "                       -3.8585e-02,  1.1613e-01,  5.2318e-02,  5.5917e-02,  1.1342e-01,\n",
              "                        2.0839e-02, -2.8812e-02, -3.6647e-02, -4.9492e-02, -1.2196e-01,\n",
              "                       -5.4384e-02,  8.4176e-02, -5.0760e-02,  7.4688e-02,  2.5210e-02,\n",
              "                       -2.5595e-02,  3.2223e-02,  8.4048e-02,  3.9075e-02,  2.3765e-02,\n",
              "                        9.0590e-02, -1.1066e-01, -8.3456e-02, -5.7707e-02,  1.9956e-02,\n",
              "                       -1.6777e-02, -8.0632e-02, -9.7592e-02, -3.8804e-02,  3.0956e-02,\n",
              "                        1.2602e-02, -1.1065e-01,  3.5721e-02,  8.6408e-02,  3.4948e-02,\n",
              "                        9.5907e-02,  9.7526e-02, -4.6713e-02, -1.3025e-01, -9.6628e-02,\n",
              "                       -1.0722e-01,  3.8779e-02,  4.8771e-03, -7.0029e-02, -4.1530e-02,\n",
              "                        3.4556e-02,  1.2770e-01,  4.2687e-02, -3.5910e-02, -3.2575e-02,\n",
              "                       -2.5684e-02,  1.4620e-02,  2.7858e-02, -1.5313e-02, -1.0379e-02,\n",
              "                        9.8309e-02,  9.0264e-02, -1.3591e-01, -8.7169e-02, -8.4641e-02,\n",
              "                        2.1873e-02, -1.3210e-01, -5.5721e-02,  8.4516e-02, -8.4946e-02,\n",
              "                        4.8820e-02,  2.4642e-02, -9.1821e-02,  8.2787e-02, -7.9834e-02,\n",
              "                        7.8168e-02, -1.0068e-01,  1.1152e-01, -1.1543e-01,  1.1165e-01,\n",
              "                       -1.0844e-01, -8.3155e-02,  9.3674e-02, -2.6952e-02, -3.5233e-02,\n",
              "                       -1.1378e-01, -1.0535e-01,  6.3797e-02,  9.6060e-02,  6.2849e-02,\n",
              "                       -1.1871e-01, -9.2035e-02, -8.4444e-02, -1.6848e-03, -5.7210e-02,\n",
              "                       -9.5969e-02,  4.6952e-02,  3.5259e-02,  1.7355e-02, -1.1264e-01],\n",
              "                      [-1.3177e-01, -8.4718e-02, -1.3222e-01, -1.4008e-02,  8.6017e-02,\n",
              "                       -1.0563e-01, -7.5908e-02,  2.3768e-02,  9.6922e-02, -6.2754e-02,\n",
              "                       -6.1597e-02, -5.6325e-02,  6.4570e-03,  1.2710e-01,  1.7803e-02,\n",
              "                       -7.6999e-02, -2.3729e-02, -1.0329e-01, -3.5279e-02, -3.2889e-02,\n",
              "                        4.9368e-02,  1.0787e-02,  1.2609e-03, -4.6543e-03, -9.8929e-03,\n",
              "                       -2.7975e-02,  5.1298e-02, -8.3841e-02,  1.0306e-01, -5.9082e-02,\n",
              "                       -5.6089e-02, -1.2054e-01,  1.4461e-02, -9.4970e-02,  9.7816e-02,\n",
              "                        1.3087e-01,  5.0454e-03,  2.8288e-02,  9.0724e-02, -5.1215e-02,\n",
              "                       -9.1640e-02, -7.9743e-02,  1.1492e-01, -5.3341e-02,  1.3967e-01,\n",
              "                        3.4614e-02,  1.1108e-01, -1.0338e-02, -1.1197e-01, -2.1610e-02,\n",
              "                       -1.9096e-02, -3.3444e-02,  3.1169e-02, -9.2128e-02, -5.5149e-03,\n",
              "                        1.2454e-01, -5.6821e-02, -7.1651e-02,  5.2826e-02,  9.2247e-02,\n",
              "                        7.7623e-02, -8.7774e-02,  1.0644e-01,  1.0594e-01, -3.3662e-02,\n",
              "                       -7.2737e-02,  4.4257e-02,  1.2478e-01,  6.9618e-03, -3.6069e-02,\n",
              "                       -9.5430e-03, -1.0392e-01, -3.7891e-02, -1.1678e-01,  1.6714e-03,\n",
              "                        7.4491e-02, -4.4771e-02,  9.5787e-02,  8.6383e-02,  3.9844e-02,\n",
              "                        1.8160e-02,  1.1383e-01,  4.4733e-02,  1.0455e-01,  7.0811e-02,\n",
              "                        7.8760e-03,  7.4434e-02,  2.0173e-02,  6.8157e-02,  5.9565e-02,\n",
              "                       -6.0383e-02,  4.9494e-02,  1.2252e-01,  4.9018e-02,  1.0038e-01,\n",
              "                        6.6914e-02,  1.4086e-01, -1.9303e-02, -3.3778e-02, -1.1469e-01,\n",
              "                        1.1535e-01,  9.1427e-02, -1.0233e-01,  8.8607e-02,  1.1887e-01,\n",
              "                       -5.4056e-02,  1.7283e-02,  1.8579e-02, -9.5673e-02, -8.8857e-02,\n",
              "                        1.2042e-01,  1.1826e-01,  2.1549e-02, -1.0516e-01,  7.9558e-02,\n",
              "                       -7.9465e-02, -6.6171e-02,  1.1939e-01,  8.3246e-02, -4.3399e-02,\n",
              "                        1.3045e-01, -5.3516e-02, -5.9834e-02,  7.5849e-02, -8.3851e-02,\n",
              "                       -7.1417e-02, -8.3834e-02, -4.1909e-02,  3.9983e-02, -1.0492e-01,\n",
              "                       -6.6747e-02,  7.2093e-02, -8.9738e-02, -3.8608e-02, -1.1047e-01,\n",
              "                        5.1489e-03,  4.0604e-02, -1.0940e-02,  2.5280e-02, -5.3539e-02,\n",
              "                       -1.0659e-01, -7.4627e-02,  9.9547e-02, -6.1792e-02,  6.1300e-02,\n",
              "                       -4.3821e-02, -7.4355e-02,  1.0584e-01, -7.2752e-02, -9.1239e-02,\n",
              "                        4.0506e-02,  5.1860e-02,  1.0729e-01, -1.1249e-02, -7.6858e-02,\n",
              "                       -7.8017e-02,  6.3218e-02, -1.9145e-02, -6.3008e-02,  1.0727e-01,\n",
              "                       -4.9716e-02, -6.3764e-02,  4.2360e-02,  3.3497e-02,  1.0168e-01,\n",
              "                        1.0921e-01, -1.0343e-01, -1.0329e-01, -8.4989e-02, -2.9055e-02,\n",
              "                        1.0260e-01,  1.0776e-01,  3.5181e-02, -8.2889e-02, -2.7317e-02,\n",
              "                       -3.1916e-02, -3.7170e-02, -8.3090e-02,  1.1151e-01,  9.0232e-02,\n",
              "                        5.2992e-02, -1.0776e-01,  5.5371e-02, -3.5952e-02,  6.0206e-02,\n",
              "                        6.0599e-02,  1.2264e-01,  1.1816e-01, -5.5219e-03,  1.1568e-01,\n",
              "                       -1.8755e-02,  3.1999e-02,  8.5015e-03, -7.9967e-02, -9.9712e-02,\n",
              "                        1.3355e-02,  3.4720e-02, -6.1665e-02, -8.5930e-02, -5.1647e-02,\n",
              "                       -4.2340e-02,  5.5521e-02,  4.3193e-03, -5.2435e-02,  7.2917e-02,\n",
              "                       -8.6118e-02, -2.2623e-02, -1.0857e-01, -1.4481e-02, -5.7351e-03,\n",
              "                        3.9072e-02,  1.1859e-01, -1.1044e-03, -1.1304e-01, -7.4127e-02,\n",
              "                       -1.1651e-01,  6.0429e-02, -5.4813e-02,  6.2920e-02, -1.0728e-01,\n",
              "                       -2.9005e-02,  9.9712e-02,  7.0354e-02,  9.6567e-02, -6.3019e-02,\n",
              "                       -1.0802e-02,  5.0519e-02,  6.5246e-02, -1.1253e-01, -1.7823e-02,\n",
              "                       -1.0796e-01,  1.0544e-01, -1.1491e-01,  9.0800e-02,  5.1673e-02,\n",
              "                        1.6069e-02, -9.7217e-02,  1.2662e-01,  8.8667e-02, -8.8962e-02,\n",
              "                        6.7248e-03,  7.3648e-02, -2.8131e-02,  4.8166e-02,  1.2704e-01,\n",
              "                        8.5506e-02,  1.2826e-01,  4.0652e-02, -9.4371e-02,  8.6042e-02,\n",
              "                       -7.0670e-02,  3.3220e-02, -6.1969e-03,  8.5661e-02, -2.7451e-02,\n",
              "                        8.3573e-02, -5.5836e-02, -1.0986e-01,  1.6131e-02, -5.7888e-02,\n",
              "                       -7.1809e-02,  9.7062e-02,  4.3135e-02,  4.1263e-02,  3.3812e-02,\n",
              "                        7.7225e-02, -6.4135e-02,  1.3366e-01,  9.7386e-02, -2.7007e-02,\n",
              "                       -1.3094e-01,  6.6845e-02, -6.1087e-02, -6.4472e-02, -4.6218e-02,\n",
              "                        3.0598e-02, -8.4807e-02, -2.8663e-02,  1.0648e-01, -7.6217e-02,\n",
              "                        7.6456e-02,  7.5692e-02, -2.5073e-02, -2.3275e-02,  8.0473e-02,\n",
              "                       -7.0510e-02, -3.2290e-02, -5.5368e-02, -7.6401e-02,  9.0473e-02,\n",
              "                        4.5987e-03, -2.1255e-02, -2.9448e-02, -1.2031e-01, -1.0199e-01,\n",
              "                       -1.5755e-02,  1.2234e-01,  3.9209e-02, -7.6831e-02, -1.2466e-01,\n",
              "                        6.0997e-02, -3.2249e-02, -1.1201e-01,  6.7025e-02,  7.6265e-02,\n",
              "                        3.5442e-02, -8.9495e-02, -2.4981e-02,  5.1754e-02, -6.1956e-03,\n",
              "                        7.3095e-02, -1.0093e-01,  1.1398e-01, -1.0919e-01, -1.1894e-02,\n",
              "                       -9.2649e-02, -1.0397e-01,  1.2586e-01,  5.1031e-02,  5.8109e-02,\n",
              "                        3.8707e-02,  7.9394e-03,  7.7178e-02, -5.5629e-02, -9.5288e-02,\n",
              "                       -7.9252e-02, -2.3224e-02, -3.3977e-02, -1.2288e-01,  4.6341e-02,\n",
              "                       -4.0373e-02,  3.6249e-02, -1.2582e-01,  1.1869e-01,  2.1170e-02,\n",
              "                       -9.0080e-02, -1.3200e-01, -1.5659e-02, -1.1029e-01,  1.0108e-01,\n",
              "                        1.3049e-01,  1.0129e-01, -5.9113e-02,  4.7459e-03, -5.0075e-02,\n",
              "                       -2.8086e-02,  6.9564e-02, -6.9871e-02, -8.3581e-02,  3.2765e-02,\n",
              "                       -8.6071e-02,  1.1143e-01,  1.0014e-02, -1.0623e-02, -1.2596e-01,\n",
              "                       -6.4139e-02, -4.3678e-02,  2.7979e-02, -3.2684e-02, -2.4527e-02,\n",
              "                       -3.0315e-02,  1.1433e-01,  1.2265e-02, -9.2612e-02, -8.3312e-02,\n",
              "                        1.1027e-01,  3.6133e-02,  1.1804e-01, -6.1398e-02,  8.8709e-02,\n",
              "                       -9.9894e-02, -1.2348e-01,  5.2812e-02, -1.0140e-01,  7.3443e-02,\n",
              "                       -4.1603e-02, -2.8961e-02,  1.1913e-01,  1.2335e-01,  9.0584e-02,\n",
              "                       -7.6596e-02, -1.0327e-01,  8.1564e-02,  9.2470e-02, -6.7664e-02,\n",
              "                       -9.1123e-02, -1.1283e-01,  8.4854e-02, -1.2026e-01,  1.0497e-01,\n",
              "                       -1.1451e-01,  9.4317e-02,  8.5218e-02,  7.1011e-02,  8.9596e-02,\n",
              "                       -9.1760e-02,  7.7336e-02, -5.4740e-02, -3.7530e-02, -1.0807e-01,\n",
              "                       -2.0612e-02, -3.9144e-02,  7.7491e-02, -3.6546e-02, -9.4110e-02,\n",
              "                        3.6248e-02, -9.7691e-02, -4.6325e-02, -2.6977e-02, -1.3016e-01,\n",
              "                       -5.7123e-03,  3.4942e-02,  6.7420e-03,  9.9411e-03,  6.6306e-02,\n",
              "                        5.4521e-02, -8.5630e-02,  4.0460e-02, -1.0790e-01, -5.2759e-02,\n",
              "                        5.5691e-02, -8.6315e-02, -2.7757e-02, -4.8418e-02, -3.3702e-02,\n",
              "                       -1.1433e-01,  1.0259e-01,  7.8354e-02,  3.9140e-02, -6.4151e-02,\n",
              "                        7.0633e-02,  1.3238e-02,  8.2784e-02,  7.1980e-02,  2.1003e-03,\n",
              "                        2.7958e-02,  9.6236e-02, -5.6349e-02, -7.5291e-02, -9.5574e-02,\n",
              "                       -8.5163e-02, -9.7875e-02,  1.5281e-02,  5.1200e-02,  7.5047e-02,\n",
              "                        1.0502e-01, -2.4747e-02,  2.0421e-02,  1.2490e-01,  8.3841e-02,\n",
              "                       -8.4134e-02, -6.2188e-02, -1.0569e-01,  5.5834e-02,  3.5342e-02,\n",
              "                       -4.6868e-03,  3.8123e-02, -1.6454e-02,  6.2432e-02,  4.8015e-02,\n",
              "                       -7.7894e-02, -1.2218e-01,  6.0136e-02,  9.0727e-02,  6.4148e-02,\n",
              "                       -6.7480e-02,  1.0936e-01,  3.7329e-02, -9.3635e-02,  2.2718e-02,\n",
              "                        5.8331e-03,  2.4815e-02,  8.2092e-02, -4.4410e-02,  1.0612e-01,\n",
              "                       -1.1899e-01,  6.0919e-02, -9.3369e-02,  1.2084e-01, -6.9207e-02,\n",
              "                        5.7172e-02,  9.6559e-02, -9.2780e-02,  4.9136e-02,  7.8974e-02,\n",
              "                        1.3674e-01,  9.1947e-02, -7.4216e-02, -1.2269e-01, -2.5072e-02,\n",
              "                        1.1790e-01,  8.3564e-02,  1.1229e-01, -2.1367e-02,  5.2936e-02,\n",
              "                        3.3589e-02, -1.1043e-01, -7.9462e-02, -7.9505e-02,  6.1920e-02]])),\n",
              "             ('fc3.bias', tensor([-0.0446, -0.0397]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfseKPHg0hOG"
      },
      "source": [
        "torch.save(model.state_dict(),'text_classifier_pytorch')"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ4iBouH0pkb",
        "outputId": "09ad583a-3241-49fa-f25e-e2810ac5f120"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  text_classifier_pytorch  tfidfmodel.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwKFztcGHdRN"
      },
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "leF7ttuWIBGh",
        "outputId": "7bd2120b-811b-492d-84e5-2672be0fbdbf"
      },
      "source": [
        "files.download('text_classifier_pytorch')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e8e6e9f8-d1e1-4c0e-b11e-1dbac06be6ed\", \"text_classifier_pytorch\", 1944129)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5i97IE-35t3"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMvo1EXgILg1"
      },
      "source": [
        "with open('tfidfmodel.pickle','wb') as file:\n",
        "    pickle.dump(vectorizer,file)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iCoZZiLY32Cf",
        "outputId": "7ff4f9c3-dd88-4ca9-f2b0-04d89d2c1ef1"
      },
      "source": [
        "files.download('tfidfmodel.pickle')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a8c8eb19-8436-4c10-849b-53ad20da306a\", \"tfidfmodel.pickle\", 49660)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfBgQDtc39yV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}