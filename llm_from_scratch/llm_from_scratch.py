# -*- coding: utf-8 -*-
"""llm_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QrNV9wrD8_oz6PzCqefPAhTPkQokqevs
"""

!pip install transformers datasets torch

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from datasets import load_dataset

# Step 1: Load the dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1%]")  # use only 1% for simplicity

# Step 2: Initialize Tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

def tokenize_function(example):
    return tokenizer(example['text'], truncation=True, padding="max_length", max_length=50)

# Tokenize the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets.set_format(type='torch', columns=['input_ids'])

# Step 3: Define Dataset and DataLoader for Language Modeling
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, tokenized_datasets, seq_len=5):
        self.seq_len = seq_len
        self.input_ids = tokenized_datasets['input_ids']

    def __len__(self):
        return sum(len(line) - self.seq_len for line in self.input_ids)

    def __getitem__(self, idx):
        line = self.input_ids[idx // (len(self.input_ids) - self.seq_len)]
        x = line[idx % (len(line) - self.seq_len): idx % (len(line) - self.seq_len) + self.seq_len]
        y = line[idx % (len(line) - self.seq_len) + self.seq_len]
        return x, y

dataset = TextDataset(tokenized_datasets)
data_loader = DataLoader(dataset, batch_size=2, shuffle=True)

# Step 4: Define the Simple Transformer model
class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, embed_size=32, num_heads=2, num_layers=2, max_seq_len=50):
        super(SimpleTransformer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.position_encoding = nn.Parameter(torch.randn(max_seq_len, embed_size))  # Max sequence length
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(embed_size, num_heads), num_layers
        )
        self.fc = nn.Linear(embed_size, vocab_size)

    def forward(self, x):
        seq_len = x.size(1)  # Get the current sequence length
        position_encoding = self.position_encoding[:seq_len, :].unsqueeze(0)  # Adjust position encoding dynamically
        x = self.embedding(x) + position_encoding  # Add position encoding to embeddings
        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)  # Transpose for transformer
        x = self.fc(x[:, -1])  # Predict the next word (last position)
        return x

# Model, optimizer, and loss function
model = SimpleTransformer(vocab_size=tokenizer.vocab_size)
optimizer = Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# Step 5: Train the Model
for epoch in range(5):  # Fewer epochs for demonstration
    total_loss = 0
    for x, y in data_loader:
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss/len(data_loader):.4f}")

def generate_text(model, tokenizer, start_text, max_len=10):
    model.eval()

    # Tokenize the input text
    tokens = tokenizer.encode(start_text, return_tensors='pt')

    generated = tokens.clone()

    for _ in range(max_len):
        with torch.no_grad():
            output = model(generated)
            next_token = torch.argmax(output, dim=-1)[-1].unsqueeze(0)  # Get the last predicted token
            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)  # Append the new token

    return tokenizer.decode(generated.squeeze().tolist())

# Example usage
print("Generated Text:", generate_text(model, tokenizer, "the quick brown"))

